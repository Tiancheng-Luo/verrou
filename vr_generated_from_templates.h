//Generated by './generateBackendInterOperator.py'
// generation of operation cast backend verrou 


static VG_REGPARM(3) Int vr_verroucast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_verrou_cast_double_to_float(*arg1, &res,backend_verrou_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation cast backend mcaquad 


static VG_REGPARM(3) Int vr_mcaquadcast64FTo32F (Long a) {
  double *arg1 = (double*)(&a);
  float res;
  interflop_mcaquad_cast_double_to_float(*arg1, &res,backend_mcaquad_context);
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation add backend verrou 


static VG_REGPARM(2) Long vr_verrouadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrouadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrouadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrouadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_verrou_add_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation sub backend verrou 


static VG_REGPARM(2) Long vr_verrousub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrousub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrousub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrousub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_verrou_sub_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation mul backend verrou 


static VG_REGPARM(2) Long vr_verroumul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroumul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroumul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroumul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_verrou_mul_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation div backend verrou 


static VG_REGPARM(2) Long vr_verroudiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res+1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroudiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroudiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroudiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_verrou_div_float(arg1[i], arg2[i], res+i, backend_verrou_context);
  }
}


// generation of operation add backend mcaquad 


static VG_REGPARM(2) Long vr_mcaquadadd64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadadd32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_mcaquad_add_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


// generation of operation sub backend mcaquad 


static VG_REGPARM(2) Long vr_mcaquadsub64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadsub32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_mcaquad_sub_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


// generation of operation mul backend mcaquad 


static VG_REGPARM(2) Long vr_mcaquadmul64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_mul_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_mul_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_mul_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_mul_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadmul32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_mul_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_mul_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_mcaquad_mul_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


// generation of operation div backend mcaquad 


static VG_REGPARM(2) Long vr_mcaquaddiv64F (Long a, Long b) {
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double res;
  interflop_mcaquad_div_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx2(/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  double arg1[2] = {*((double*)(&aLo)),*((double*)(&aHi))} ;
  double arg2[2] = {*((double*)(&bLo)),*((double*)(&bHi))} ;
  double* res=(double*) output;
  interflop_mcaquad_div_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_div_double(arg1[1], arg2[1], res+1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx4 (/*OUT*/V256* output,
                                           ULong b0, ULong b1, ULong b2,ULong b3) {

  double arg2[4] = {*((double*)(&b0)),*((double*)(&b1)), *((double*)(&b2)),*((double*)(&b3))} ;
  double* res=(double*) output;
  for(int i=0; i<4; i++){
     interflop_mcaquad_div_double(arg1CopyAvxDouble[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquaddiv32F (Long a, Long b) {
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float res;
  interflop_mcaquad_div_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int*)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx8 (/*OUT*/V256* output,
					   ULong b0, ULong b1, ULong b2,ULong b3) {
  V256 reg2;   reg2.w64[0]=b0;   reg2.w64[1]=b1;   reg2.w64[2]=b2;   reg2.w64[3]=b3;
  float* res=(float*) output;
  float* arg1=arg1CopyAvxFloat;
  float* arg2=(float*) &reg2;
  for(int i=0; i<8; i++){
     interflop_mcaquad_div_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx4 (/*OUT*/V128* output, ULong aHi, ULong aLo, ULong bHi,ULong bLo) {
  V128 reg1; reg1.w64[0]=aLo; reg1.w64[1]=aHi;
  V128 reg2; reg2.w64[0]=bLo; reg2.w64[1]=bHi;

  float* res=(float*) output;
  float* arg1=(float*) &reg1;
  float* arg2=(float*) &reg2;

  for(int i=0; i<4;i++){
     interflop_mcaquad_div_float(arg1[i], arg2[i], res+i, backend_mcaquad_context);
  }
}


// generation of operation madd backend verrou 
//FMA Operator
static VG_REGPARM(3) Long vr_verroumadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2,  *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
// generation of operation msub backend verrou 
//FMA Operator
static VG_REGPARM(3) Long vr_verroumsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double*)(&a);
  double *arg2 = (double*)(&b);
  double *arg3 = (double*)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  double res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long*)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float*)(&a);
  float *arg2 = (float*)(&b);
  float *arg3 = (float*)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, - *arg3, &res, backend_verrou_context);
#else
  float res=0.;
  VG_(tool_panic) ( "Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int*)(&res);
  return *d;
}
